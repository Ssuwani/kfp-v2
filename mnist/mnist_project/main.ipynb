{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2db73473-2c4a-45f4-ac6b-616681edb721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/vertexAI/2_mnist_project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c8cf457-d6b0-43ee-823a-0aff78acdd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘components/load_data/src’: File exists\n",
      "mkdir: cannot create directory ‘components/train_linear/src’: File exists\n",
      "mkdir: cannot create directory ‘components/train_cnn/src’: File exists\n",
      "mkdir: cannot create directory ‘components/evaluate_models/src’: File exists\n",
      "mkdir: cannot create directory ‘components/deploy_model/src’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir components/load_data/src\n",
    "!mkdir components/train_linear/src\n",
    "!mkdir components/train_cnn/src\n",
    "!mkdir components/evaluate_models/src\n",
    "!mkdir components/deploy_model/src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cebcd4-0525-42ca-9e3a-f744278a8319",
   "metadata": {},
   "source": [
    "<img src=\"demo.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783b88f-d70b-4d6a-9e6c-4e3919ce3b66",
   "metadata": {},
   "source": [
    "### 1. Component / load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1c180ddd-3b99-4ef2-8ecf-692399e4dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/load_data/src/load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/load_data/src/load_data.py\n",
    "from kfp.v2.dsl import *\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"numpy\"],\n",
    "    output_component_file=\"load_data.yaml\"\n",
    ")\n",
    "def load_data(\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    \n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (train_x, train_y), (test_x, test_y) = mnist.load_data()\n",
    "    train_x = train_x / 255.0\n",
    "    test_x = test_x / 255.0\n",
    "    \n",
    "    with open(dataset.path, \"wb\") as f:\n",
    "        np.savez(\n",
    "            f,\n",
    "            train_x=train_x,\n",
    "            train_y=train_y,\n",
    "            test_x=test_x,\n",
    "            test_y=test_y\n",
    "        )\n",
    "    print(f\"Saved on : {dataset.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f5b573-16d9-47c4-bf84-ebee8bb53327",
   "metadata": {},
   "source": [
    "## 2. Component / train linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb35acc2-0e6f-432f-8306-a6563a2f8c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/train_linear/src/train_linear.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/train_linear/src/train_linear.py\n",
    "from kfp.v2.dsl import *\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"numpy\"],\n",
    "    output_component_file=\"train_linear.yaml\"\n",
    ")\n",
    "def train_linear(\n",
    "    dataset: Input[Dataset],\n",
    "    output_model: Output[Model],\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    with open(dataset.path, \"rb\") as f:\n",
    "        mnist = np.load(f)\n",
    "        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n",
    "        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n",
    "    print(f\"train x shape: {train_x.shape}\")\n",
    "    print(f\"train y shape: {train_y.shape}\")\n",
    "    print(f\"test x shape: {test_x.shape}\")\n",
    "    print(f\"test y shape: {test_y.shape}\")\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    model.fit(train_x, train_y)\n",
    "    loss, acc = model.evaluate(test_x, test_y)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\",(acc * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Tensorflow\")\n",
    "    metrics.log_metric(\"Model\", \"LinearModel\")\n",
    "    metrics.log_metric(\"dataset_size\", len(train_x))\n",
    "    \n",
    "    model.save(output_model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177bed08-0e10-4d14-895d-a74db3764b04",
   "metadata": {},
   "source": [
    "## 3. Component / train cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "071ce4cc-6bf0-4d0e-b113-8934c8be28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/train_cnn/src/train_cnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/train_cnn/src/train_cnn.py\n",
    "from kfp.v2.dsl import *\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"numpy\"],\n",
    "    output_component_file=\"train_cnn.yaml\"\n",
    ")\n",
    "def train_cnn(\n",
    "    dataset: Input[Dataset],\n",
    "    output_model: Output[Model],\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    with open(dataset.path, \"rb\") as f:\n",
    "        mnist = np.load(f)\n",
    "        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n",
    "        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n",
    "    train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "    test_x = test_x.reshape(-1, 28, 28, 1)\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')            \n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['acc']\n",
    "    )\n",
    "    model.fit(train_x, train_y)\n",
    "    loss, acc = model.evaluate(test_x, test_y)\n",
    "\n",
    "    metrics.log_metric(\"accuracy\",(acc * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Tensorflow\")\n",
    "    metrics.log_metric(\"Model\", \"CNNModel\")\n",
    "    metrics.log_metric(\"dataset_size\", len(train_x))\n",
    "    \n",
    "    model.save(output_model.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7007e19-25a5-4fc8-840a-1a19bcb12b9e",
   "metadata": {},
   "source": [
    "## 4. Component / Evalute models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c24a21fa-1df8-4216-90ba-c825278f757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/evaluate_models/src/evaluate_models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/evaluate_models/src/evaluate_models.py\n",
    "from kfp.v2.dsl import *\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"numpy\"],\n",
    "    output_component_file=\"evaluate_models.yaml\"\n",
    ")\n",
    "def evaluate_models(\n",
    "    dataset: Input[Dataset],\n",
    "    model_linear: Input[Model],\n",
    "    model_cnn: Input[Model],\n",
    "    model_best: Output[Model]\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    from tensorflow.keras.models import clone_model\n",
    "    \n",
    "    with open(dataset.path, \"rb\") as f:\n",
    "        mnist = np.load(f)\n",
    "        train_x, train_y = mnist[\"train_x\"], mnist[\"train_y\"]\n",
    "        test_x, test_y = mnist[\"test_x\"], mnist[\"test_y\"]\n",
    "    \n",
    "    # evaluate Linear Model\n",
    "    linear_model = tf.keras.models.load_model(model_linear.path)\n",
    "    linear_loss, linear_acc = linear_model.evaluate(test_x, test_y)\n",
    "    \n",
    "    # evaluate CNN Model\n",
    "    test_x = test_x.reshape(-1, 28, 28, 1)\n",
    "    cnn_model = tf.keras.models.load_model(model_cnn.path)\n",
    "    cnn_loss, cnn_acc = cnn_model.evaluate(test_x, test_y)\n",
    "    \n",
    "    best_model = cnn_model if cnn_acc > linear_acc else linear_model\n",
    "    best_model.save(model_best.path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e33496-72c7-43ed-98bd-64303570c933",
   "metadata": {},
   "source": [
    "## 5. Deploy Model on Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e7d520ae-398d-42a3-8898-9a17a3313c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/deploy_model/src/deploy_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/deploy_model/src/deploy_model.py\n",
    "from kfp.v2.dsl import *\n",
    "\n",
    "@component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\n",
    "    output_component_file=\"deploy_model.yaml\"\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project, location=region) # 1\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload( # 2\n",
    "        display_name=\"simple-mnist-pipeline\",\n",
    "        artifact_uri = model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\") # 3\n",
    "\n",
    "    # Save data to the output params\n",
    "    vertex_endpoint.uri = endpoint.resource_name # 4\n",
    "    vertex_model.uri = deployed_model.resource_name "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2756479e-71cc-469b-ba35-72a6856f6068",
   "metadata": {},
   "source": [
    "### 100. define Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "00bf09d9-8f80-40aa-b66c-749fef2d19ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipelines/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipelines/pipeline.py\n",
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp import components\n",
    "\n",
    "PROJECT_ID = \"abstract-flame-330901\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"mnist-pipeline\",\n",
    "    description=\"mnist pipeline tutorial\",\n",
    "    pipeline_root=\"gs://suwan\"\n",
    ")\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    load_data = components.load_component_from_file(\"load_data.yaml\")\n",
    "    train_linear = components.load_component_from_file(\"train_linear.yaml\")\n",
    "    train_cnn = components.load_component_from_file(\"train_cnn.yaml\")\n",
    "    evaluate_models = components.load_component_from_file(\"evaluate_models.yaml\")\n",
    "    deploy_model = components.load_component_from_file(\"deploy_model.yaml\")\n",
    "    load_data_task = load_data()\n",
    "    train_linear_task = train_linear(\n",
    "        dataset=load_data_task.outputs[\"dataset\"]\n",
    "    )\n",
    "    train_cnn_task = train_cnn(\n",
    "        dataset=load_data_task.outputs[\"dataset\"]\n",
    "    )\n",
    "    evaluate_models_task = evaluate_models(\n",
    "        dataset=load_data_task.outputs[\"dataset\"],\n",
    "        model_linear=train_linear_task.outputs[\"output_model\"],\n",
    "        model_cnn=train_cnn_task.outputs[\"output_model\"]\n",
    "    )\n",
    "    deploy_task = deploy_model(\n",
    "        model=evaluate_models_task.outputs[\"model_best\"],\n",
    "        project=project,\n",
    "        region=region\n",
    "    )\n",
    "\n",
    "client = kfp.Client(\"https://22767e8e71dc72a3-dot-us-central1.pipelines.googleusercontent.com/\")\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    pipeline,\n",
    "    arguments={},\n",
    "    mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE,\n",
    ")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a498fa10-c9b4-48f2-b6e2-677e2cf1b7fc",
   "metadata": {},
   "source": [
    "## 100.1 Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "186a3adf-e774-4ef6-abae-d7d2c805696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python components/load_data/src/load_data.py\n",
    "!python components/train_linear/src/train_linear.py\n",
    "!python components/train_cnn/src/train_cnn.py\n",
    "!python components/evaluate_models/src/evaluate_models.py\n",
    "!python components/deploy_model/src/deploy_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f1dc941-dac4-453c-8d2d-f4fec9c06eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/compiler/compiler.py:79: UserWarning: V2_COMPATIBLE execution mode is at Beta quality. Some pipeline features may not work as expected.\n",
      "  warnings.warn('V2_COMPATIBLE execution mode is at Beta quality.'\n"
     ]
    }
   ],
   "source": [
    "!python pipelines/pipeline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482e5d8-e03e-42f3-8f5e-8dbfdba20317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3bf78-9616-42bb-8b55-0790d389348b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
